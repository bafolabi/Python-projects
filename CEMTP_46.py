# -*- coding: utf-8 -*-
"""CEMTP 46.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SMRfHaIK_UtXlk7xMAkohxvEjxw_XJzw

# Mount drive
"""

drive.mount('/content/drive')

"""# **Import Libraries**"""

from google.colab import drive
import string
import re
import pandas as pd
import random
import matplotlib.pyplot as plt
from nltk.tokenize import sent_tokenize
import string
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
from nltk.stem import WordNetLemmatizer
import pickle
from wordcloud import WordCloud
from sklearn.metrics import confusion_matrix, f1_score




nltk.download("wordnet")
nltk.download("omw-1.4")

"""# Dataset Statistics, Visualization and Preprocessing"""

sentiment_data = pd.read_csv("/content/drive/MyDrive/Google Colab/training.1600000.processed.noemoticon.csv", encoding='ISO-8859-1')
sentiment_data.columns = ['Target', 'ID', 'Date','Flag','User','Text']

sentiment_data["Target"].replace(4,1, inplace=True)

new_data = sentiment_data[650000:950000]
new_data

#data preprocessing 
def clean_text(text):
    # Remove Twitter #tags and @usernames
    text = re.sub(r'#\w+', '', text)
    text = re.sub(r'@\w+', '', text)
    
    # Remove URLs
    text = re.sub(r'http\S+', '', text)
    
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    
    # Convert to lowercase
    text = text.lower()
    
    return text

#Extract the sentiment and sentence columns
text_data = []
for data in new_data["Text"]:
    if isinstance(data, str):
        text_data.append(clean_text(data))
    else:
        text_data.append('')
target_data = new_data["Target"]

target_data.value_counts()

"""# Building ML Models

## In this experiment, we considered the `SVM` algorithm on `regex`,`TFDIFvectorizer`, and `lemmatization` fine-tuning with `pickle` file
"""

#Lemmatization
lemmatizer = WordNetLemmatizer()
lemmatized_messages = []

for message in text_data:
    lemmatized_message = " ".join([lemmatizer.lemmatize(word,pos="v") for word in message.split()])
    lemmatized_messages.append(lemmatized_message)

# Convert text data to numerical feature vectors
vectorizer = TfidfVectorizer()
vectorizer_fit = vectorizer.fit(lemmatized_messages)
X = vectorizer.transform(lemmatized_messages)


#LOAD TFIDF MODEL TO PICKLE
pickle.dump(vectorizer_fit,open("vectorizer.pkl","wb"))
Tfidf_pickle = pickle.load(open('vectorizer.pkl','rb'))


# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, target_data, test_size=0.3, random_state=42)

# Train the model
model = svm.SVC(kernel='linear')
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Compute the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy*100))

f1 = f1_score(y_test, y_pred, average='weighted')
cm = confusion_matrix(y_test, y_pred)
print("F1 score: {:.2f}".format(f1))
print("Confusion matrix:\n", cm)

pickle.dump(model,open("model.pkl","wb"))

# Loading model to compare the results
model_pickle = pickle.load(open('model.pkl','rb'))

#Test Accuracy
test_data1 = sentiment_data[10000:50000]
test_data2 = sentiment_data[1200000:1250000]
real_test = pd.concat([test_data1, test_data2])
real_test

text_data = []
for data in real_test["Text"]:
    if isinstance(data, str):
        text_data.append(clean_text(data))
    else:
        text_data.append('')

model_pickle = pickle.load(open('model.pkl','rb'))
Tfidf_pickle = pickle.load(open('vectorizer.pkl','rb'))

lemmatizer = WordNetLemmatizer()
lemmatized_messages = []

for message in text_data:
    lemmatized_message = " ".join([lemmatizer.lemmatize(word,pos="v") for word in message.split()])
    lemmatized_messages.append(lemmatized_message)


X = Tfidf_pickle.transform(lemmatized_messages)

y = model_pickle.predict(X)

#Model test accuracy
y_test = real_test["Target"]
y_pred = y

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy*100))

f1 = f1_score(y_test, y_pred, average='weighted')
cm = confusion_matrix(y_test, y_pred)
print("F1 score: {:.2f}".format(f1))
print("Confusion matrix:\n", cm)

"""## Here we test the accuracy of the model using the `pickle` on `COVID-19 tweets data`"""

#Load COVID-19 dataset

covid_test = pd.read_csv("/content/drive/MyDrive/Google Colab/covid-19_vaccine_tweets_with_sentiment 2.csv")

covid_test

#COVID-19 Data preprocessing
text_data = []
for data in covid_test["tweet_text"]:
    if isinstance(data, str):
        text_data.append(clean_text(data))
    else:
        text_data.append('')

#Testing Model with COVID-19 dataset
model_pickle = pickle.load(open('model.pkl','rb'))
Tfidf_pickle = pickle.load(open('vectorizer.pkl','rb'))

lemmatizer = WordNetLemmatizer()
lemmatized_messages = []

for message in text_data:
    lemmatized_message = " ".join([lemmatizer.lemmatize(word,pos="v") for word in message.split()])
    lemmatized_messages.append(lemmatized_message)


X = Tfidf_pickle.transform(lemmatized_messages)

y = model_pickle.predict(X)

#Evaluation Metrics
y_pred = y
y_test = covid_test["label"]
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy*100))

f1 = f1_score(y_test, y_pred, average='weighted')
cm = confusion_matrix(y_test, y_pred)
print("F1 score: {:.2f}".format(f1))
print("Confusion matrix:\n", cm)

"""## In this experiment, we considered the `Naiye Bayes` algorithm on `regex` `TFDIFvectorizer` fine-tuning."""

#Vectorize the text data using TfidfVectorizer with unigram
cveck = TfidfVectorizer(ngram_range=(1,1))
t_x = cveck.fit_transform(text_data)


# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(t_x, target_data, test_size=0.3, random_state=42)

# Train the model
from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Compute the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy*100))

"""## In this experiment, we considered the `random forest` algorithm on `regex`, `TFDIFvectorizer` fine-tuning."""

# Vectorize the text data using TfidfVectorizer with unigram
cveck = TfidfVectorizer(ngram_range=(1,1))
t_x = cveck.fit_transform(text_data)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(t_x, target_data, test_size=0.3, random_state=42)

# Train the Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)


# Compute the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy*100))

"""## In this experiment, we considered the `Naiye Bayes` algorithm on `regex`,`countvectorizer`, and `stemming` fine-tuning."""

# Vectorize the text data using stemming
from nltk.stem import PorterStemmer

#stemmazation
stemmer= PorterStemmer()
stem_messages = []

for message in text_data:
    stemmed_message = " ".join([stemmer.stem(word) for word in message.split()])
    stem_messages.append(stemmed_message)

# Vectorize the text data using CountVectorizer
cvec = CountVectorizer()
t_x_data = cvec.fit_transform(stem_messages)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(t_x_data, target_data, test_size=0.3, random_state=42)

# Train the model
from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Compute the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy*100))